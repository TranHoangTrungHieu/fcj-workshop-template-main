[{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/","title":" Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"The Importance of Encryption and How AWS Can Help Author: Ken Beer ‚Ä¢ Published: February 12, 2025\nCategories: AWS CloudHSM, AWS Key Management Service, AWS Wickr, Security, Identity \u0026amp; Compliance\nThis article has been updated to include new AWS services and features released since the original publication in 2020.\nEncryption is a key component of a defense-in-depth security strategy for protecting your data, workloads, and digital assets. As organizations innovate and build trust with their customers, they must meet critical compliance requirements and strengthen data protection.\nHow and Why Encryption Works Encryption uses an algorithm and a key to transform readable data into ciphertext ‚Äî which can only be decrypted using the correct key.\nExample:\nHello World! ‚Üí 1c28df2b595b4e30b7b07500963dc7c\nStrong encryption systems rely on mathematical properties that make ciphertext computationally infeasible to break without the correct key.\nTherefore, protecting and managing encryption keys is central to every secure encryption solution.\nEncryption as Part of Your Security Strategy AWS uses the shared responsibility model.\nYou control access to your data, while AWS secures the underlying infrastructure.\nEncryption helps minimize risks such as:\nExcessive or misconfigured access Data exposure during transmission Unauthorized access at rest How secure is AES-256? AWS uses AES-256, a government-approved and industry-standard symmetric encryption algorithm.\nEven with future quantum computers, breaking AES-256 remains computationally infeasible.\nRequirements for an Effective Encryption Solution 1. Protecting Keys at Rest A strong encryption solution must ensure that:\nKeys never exist in plaintext outside a protected boundary Algorithms are implemented correctly and securely AWS uses HSMs (Hardware Security Modules) with:\nTamper detection and tamper response FIPS 140-2/140-3 Level 3 validation Two AWS HSM-based services: Service Description AWS KMS AWS manages HSM clusters for you AWS CloudHSM You manage your own HSMs Both support:\nCreating keys Importing keys from on-premises systems Direct encryption or envelope encryption Envelope Encryption A data encryption key (DEK) encrypts your data ‚Üí\nDEK is then encrypted using a master key stored in HSM ‚Üí\nImproves performance and reduces load on HSMs.\n2. Independent Key Management You must separate:\nKey administrators Data access administrators AWS KMS provides:\nFine-grained IAM policy control Key usage tracking via CloudTrail AWS CloudHSM provides fully independent key access policies.\nExternal Key Store (XKS) Since 2022, AWS KMS allows customers to store KMS keys in their own HSMs (on-premises or chosen locations).\nKMS proxies encryption/decryption requests to your HSM.\nThe key material never leaves your HSM.\nHowever, you become responsible for:\nDurability Availability Latency Throughput Losing the key means permanent loss of data.\nMonitoring and Auditing AWS KMS ‚Üí CloudTrail: logs all key usage AWS CloudHSM ‚Üí CloudWatch: streams audit events Encrypting Data at Rest and In Transit At Rest AWS services use AES-256 with master keys protected in HSMs.\nIn Transit AWS uses TLS 1.2 or higher, with TLS 1.3 recommended.\ns2n-tls Due to limitations in OpenSSL, AWS created s2n-tls:\nLightweight Easier to audit Verified using formal methods Open-source (Apache 2.0) s2n-quic and HTTP/3 AWS built s2n-quic in Rust to support QUIC ‚Äî the foundation of HTTP/3.\nAmazon CloudFront supports HTTP/3 using s2n-quic.\nCertificate Management AWS simplifies issuing and rotating certificates via:\nAWS Certificate Manager (ACM) AWS Private CA Keys used for certificates are protected via KMS or CloudHSM.\nCryptographic Computing (Encrypting Data in Use) Used for:\nFederated machine learning Multi-party computation AWS Clean Rooms with C3R (Cryptographic Computing for Clean Rooms) enables data collaboration while keeping all data encrypted.\nQuantum Computing and Post-Quantum Cryptography (PQC) Quantum computers may break:\nRSA ECC Diffie‚ÄìHellman But AES-256 remains secure.\nAWS Adoption of PQC AWS has integrated ML-KEM, a NIST-selected PQC algorithm, into:\nAWS-LC (FIPS 140-3 validated library) s2n-tls AWS KMS AWS Certificate Manager AWS Secrets Manager AWS Transfer Family supports post-quantum hybrid SFTP.\nEncrypt Everything, Everywhere AWS enables encryption:\nAt rest In transit In memory Examples:\nS3 encrypts new objects by default EBS uses envelope encryption Graviton2/3 processors use always-on memory encryption As part of the AWS Digital Sovereignty Pledge, AWS continues investing in advanced encryption controls.\nConclusion At AWS, security is the top priority.\nAWS helps customers control how data is encrypted, managed, and protected across their entire infrastructure.\nAWS provides:\nStrong encryption technologies Scalable key management Compliance-ready security controls If you have questions, start a thread in the AWS KMS or AWS CloudHSM forums, or contact AWS Support.\nAuthors Ken Beer ‚Äì Director of AWS Key Management Service (AWS KMS) and AWS Cryptography Libraries.\nHe has more than seven years of experience at AWS in identity and access management, encryption, and key management.\nBefore joining AWS, he led network security at Trend Micro and previously worked at Tumbleweed Communications.\nHe has presented at RSA Conference, the U.S. DoD PKI User‚Äôs Forum, and AWS re:Invent.\nZach Miller ‚Äì Principal Security Specialist Solutions Architect at AWS.\nHe specializes in data protection, security architecture, applied cryptography, and secrets management.\nHe works with AWS enterprise customers to help them strengthen security posture and reduce operational risk.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Hoang Trung Hieu\nPhone Number: 0963832382\nEmail: tranhoangtrunghieu19102004@gmail.com\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"üéØ Week 1 Objectives Understand Cloud Computing fundamentals. Set up AWS Free Tier account and Billing. Configure IAM and MFA security. Learn AWS CLI basics. Explore core AWS services. üìÖ Tasks Completed Day Task Description Start End Reference Monday Cloud Computing overview (IaaS/PaaS/SaaS) 09/08 09/08 https://youtu.be/l8isyDe-GwY Tuesday Create AWS Account, Free Tier, IAM login 09/09 09/09 https://youtu.be/mXRqgMr_97U Wednesday Regions, Availability Zones, enable MFA 09/10 09/10 https://youtu.be/HxYZAK1coOI Thursday Overview of EC2, S3, VPC, RDS, IAM 09/11 09/11 https://youtu.be/IK59Zdd1poE Friday Install AWS CLI and basic commands 09/12 09/12 https://youtu.be/HSzrWGqo3ME üß† Knowledge Gained Cloud concepts and benefits. IAM Users, Policies, MFA setup. AWS Global Infrastructure (Region/AZ). Difference between On-Premises vs Cloud. AWS CLI configuration and usage. üõ† Practical Skills Configure AWS Free Tier account. Create IAM Users and enable MFA. Run CLI commands: aws configure aws s3 ls aws ec2 describe-regions Navigate AWS Management Console. ‚úîÔ∏è Week 1 Summary This week established a strong foundation in Cloud Computing, AWS Accounts, IAM security, and CLI usage, preparing for EC2 and VPC work next week.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Aurora Time Executive Summary This proposal introduces Aurora Time, an AWS-based personal time management app designed to simplify scheduling with a clean and intuitive interface.\nIt leverages AWS Serverless architecture to ensure scalability, reliability, and cost efficiency ‚Äî providing quick ROI by minimizing infrastructure management.\nProblem Statement Current Situation Users struggle to manage daily commitments due to fragmented tools (notes, phone apps, calendars).\nExisting products are complex and enterprise-focused rather than for personal scheduling.\nAurora Time delivers a centralized, minimal, and user-friendly solution for managing personal events and routines.\nProposed Solution Aurora Time uses Amazon S3 \u0026amp; CloudFront for static web hosting and AWS Amplify for simplified deployment.\nAPI Gateway and Lambda handle backend operations, with DynamoDB as a fast, low-latency database.\nCognito secures authentication, EventBridge automates reminders, and SES sends email alerts.\nBenefits and ROI Cost: $16 ‚Äì $50/month (~$192 ‚Äì $600/year) ROI: \u0026lt; 6 months Advantage: Serverless model = no 24/7 servers, ultra-low cost, scalable instantly. Solution Architecture Aurora Time employs a fully AWS Serverless architecture that scales from one to millions of users.\nAll API calls are handled by Amazon API Gateway, processed via AWS Lambda, and stored in Amazon DynamoDB.\nAmazon EventBridge automates reminders, while AWS Amplify serves the UI protected by Amazon Cognito.\nAWS Services Used AWS Service Function AWS Lambda Executes backend logic for CRUD and reminders. Amazon API Gateway Provides secure RESTful API endpoints. Amazon DynamoDB Stores user data, events, and schedules. Amazon S3 \u0026amp; CloudFront Hosts and delivers static content. Amazon EventBridge Triggers and manages scheduled reminders. Amazon SES Sends reminder emails. AWS Amplify Handles frontend hosting and deployment. Amazon Cognito Provides secure user authentication. Implementation Plan Month 1 ‚Äì Research \u0026amp; Design: DynamoDB modeling, Serverless architecture. Month 1 ‚Äì Cost \u0026amp; POC: Use Pricing Calculator, test Cognito + DynamoDB. Month 2 ‚Äì Optimization: Tune Lambda \u0026amp; DynamoDB for efficiency. Month 2‚Äì3 ‚Äì Development \u0026amp; CI/CD: Build Lambda, integrate CodePipeline, develop React frontend, test Beta. Budget Estimation Service Description Cost (USD/month) AWS Amplify Static web hosting 0.35 Amazon S3 File storage \u0026amp; backup 0.05 CloudFront CDN (20GB) 1.70 API Gateway 30k requests 0.11 Lambda 1M requests (free) 0.00 DynamoDB 1GB data (free) 0.11 Cognito \u0026lt;1000 users 0.00 SES 500 emails 0.05 EventBridge 100k events 0.10 CloudWatch Logs 1GB log 0.10 CI/CD Pipeline 20 builds 0.00 ‚Üí Total Estimated Cost: $16 ‚Äì $50/month (~$192 ‚Äì $600/year)\nRisk \u0026amp; Mitigation Risk Impact Probability Mitigation Network latency Medium Medium Use CDN + local caching. DynamoDB design issues High Medium Conduct POC, load testing. Cost overrun Medium Low Enable AWS Budgets \u0026amp; alerts. Reminder failure High Low Monitor EventBridge \u0026amp; Lambda via CloudWatch. Expected Outcomes Simplified personal scheduling with automated reminders. Stable, low-cost, scalable Serverless system. Fully deployed CI/CD pipeline within internship period. "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"üéØ Week 2 Objectives Understand Amazon EC2. Learn VPC networking fundamentals. Practice Security Groups and NACLs. Use IAM Roles for EC2. Explore Windows workloads \u0026amp; hybrid networking. üìÖ Tasks Completed Day Task Description Start End Reference Monday Launch EC2 Linux, SSH via VSCode 09/15 09/15 https://youtu.be/ulenjQwU02g Tuesday IAM Role \u0026amp; Instance Profile for EC2 09/16 09/16 https://youtu.be/5xlKPzAwiZA Wednesday Create VPC, Subnet, Routing, IGW 09/17 09/17 https://youtu.be/OKc2V0tVQqY Thursday Security Groups \u0026amp; NACLs 09/18 09/18 https://youtu.be/QVLnEjtF1Fo Friday Windows EC2 + AD Overview 09/19 09/19 https://aws.amazon.com/windows/ üß† Knowledge Gained EC2 provisioning, AMIs and EBS storage. VPC architecture: subnets, routing, gateways. Comparison between SGs and NACLs. IAM Role assignment best practices. Windows Server workloads on AWS. üõ† Practical Skills SSH into EC2 using key pair. Create custom VPC manually. Assign IAM Role to EC2. RDP into Windows EC2 instance. Test traffic flow through SG \u0026amp; NACL. ‚úîÔ∏è Week 2 Summary This week built a deep understanding of AWS Compute and Networking, key foundations for any cloud infrastructure.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/3-blogstranslated/3.2-blog2/","title":"Blog 2 ","tags":[],"description":"","content":"Proactive Strategies to Improve Network Resilience and Maintain Business Continuity on AWS Authors: Devin Gordon \u0026amp; Henrik Balle\nPublished: July 11, 2025\nCategories: Best Practices, Disaster Response, Public Sector, Security, Identity \u0026amp; Compliance\nAmazon Web Services (AWS) recommends that organizations prepare to recover their workloads in the event of cybersecurity incidents or business continuity disruptions‚Äîsuch as technical failures or natural disasters.\nThis article provides guidance and recommended strategies for public sector organizations to leverage AWS infrastructure in order to build highly resilient cloud-based systems. AWS encourages its customers to:\nUse cybersecurity standards and AWS architectural best practices. Implement a multi-account environment. Use Infrastructure as Code (IaC) to deploy environments and workloads. Prepare a recovery account in a separate Availability Zone (AZ) or Region. Store application code, IaC templates, configuration files, and dependencies in the recovery account. Build a backup strategy that replicates data to the recovery account. Implement automated unit and full workload testing within the recovery account. Use a Recognized Cybersecurity Framework As cybersecurity threats such as ransomware continue to increase, public sector organizations are turning to frameworks like the NIST Cybersecurity Framework (CSF) 2.0 for guidance on managing cybersecurity risks.\nAlthough CSF organizes cybersecurity outcomes into six functions:\nGovern Identify Protect Detect Respond Recover ‚Ä¶it does not prescribe how these outcomes must be achieved.\nTo provide more concrete guidance:\nThe AWS Blueprint for Ransomware Defense maps specific AWS services to the CSF functions. The AWS Well-Architected Framework outlines six pillars to help architects build secure, reliable, high-performing, and efficient workloads, aligned with the AWS Shared Responsibility Model. The AWS Security Reference Architecture (AWS SRA) supplements these frameworks by helping organizations design and operate AWS security services according to AWS best practices. By combining NIST CSF and the Well-Architected Framework, this article explores essential patterns to prepare organizations for recovery from cybersecurity or disaster events.\nImplement a Multi-Account Strategy on AWS AWS recommends adopting a multi-account strategy‚Äîa best practice for building a scalable, controlled cloud environment (landing zone).\nKey enabling tools:\nAWS Organizations AWS Control Tower Landing Zone Accelerator on AWS Benefits of a multi-account strategy include:\nStronger workload and data isolation Reduced risk of unintended lateral movement Simplified automation and governance at scale AWS also recommends centrally managing user identities using AWS IAM Identity Center. External identity providers can be integrated, but during recovery situations you may need to use root credentials‚Äîalthough AWS strongly advises limiting root access to essential tasks only.\nRecently, AWS introduced centralized root credential management, improving overall security posture by eliminating long-lived credentials and preventing unauthorized recovery actions.\nBuild Your AWS Infrastructure with Automation (IaC) AWS recommends deploying all infrastructure using Infrastructure as Code (IaC)‚Äîa core DevSecOps principle.\nBenefits of IaC:\nFaster infrastructure replication Increased consistency and reproducibility Fewer configuration errors Built-in version control Faster recovery operations AWS provides multiple IaC options:\nAWS CloudFormation AWS Cloud Development Kit (AWS CDK) AWS Serverless Application Model (AWS SAM) Terraform (third-party, multi-cloud support) IaC should be parameterized using:\nResource names AWS Regions IP ranges Other environment-specific values ‚Üí enabling reuse across accounts and Regions.\nIaC code should be stored in a source code repository (e.g., GitLab).\nOrganizations can adopt automated pipelines using AWS Prescriptive Guidance DevOps Pipeline Accelerator (DPA).\nPrepare a Recovery Location AWS recommends creating dedicated recovery accounts to protect against cybersecurity incidents such as identity compromise.\nRisk scenario:\nIf an attacker gains root access or escalates privileges in the primary account, backups stored in that same account could also be compromised.\nRecovery Within the Same Region (AZ Recovery) If recovering within the same Region:\nChoose an Availability Zone that is completely isolated. Maintain consistent Availability Zone IDs (AZ IDs) across all accounts. AWS assigns different AZ names per account, so AZ IDs must be used to ensure the recovery AZ never overlaps with deployment AZs.\nMulti-Region Recovery For resilience against large-scale disasters:\nUse multi-Region recovery, such as primary workloads in us-east-1 and recovery in us-east-2. At minimum, AWS recommends backing up data to another Region. Cross-Region backup may require revisiting your AWS Key Management Service (AWS KMS) key strategy to ensure encrypted data can be decrypted in the recovery account and Region.\nNote: Inter-Region data transfer costs are higher than inter-AZ transfers.\nBuild a Backup Strategy and Implement Automated Testing AWS recommends designing a cloud-aligned backup strategy. After preparing the recovery location, organizations can use AWS Backup to store workload data in the recovery account.\nAWS Backup can also store:\nIaC templates Infrastructure configuration data Application code Application configuration files To ensure backup integrity, AWS recommends:\nAutomated testing and validation Verifying that backup data can be read Retaining multiple versions to mitigate ransomware risks Using AWS Backup Vault Lock to prevent unauthorized modifications Recovery workflows should include backups of every component required to rebuild workloads:\nIaC Infrastructure configuration AMIs Application code Application settings Testing should occur on a recurring basis. Organizations should periodically rebuild the entire workload in the recovery account using the stored backups and IaC templates.\nExisting component, integration, or user-acceptance testing scripts can be reused for these business continuity exercises.\nConclusion By following the best practices described in this article, public sector organizations can leverage AWS capabilities to prepare for and recover from business continuity events.\nFor additional guidance, contact your AWS account team or solutions architect.\nAuthors Devin Gordon Principal Solutions Architect at AWS, supporting U.S. federal civilian agencies.\nSpecializes in AWS security and enjoys helping customers deploy secure environments.\nInterests: travel, SCUBA diving, hiking, outdoor activities.\nHenrik Balle Principal Solutions Architect at AWS supporting the U.S. Public Sector.\nWorks with customers on machine learning, security, and governance at scale.\nInterests: long-distance road biking, motorcycling, and home improvement projects.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section provides a list and short introduction to the blogs you have translated. üëâ Blog 1 ‚Äì The Importance of Encryption and How AWS Helps This blog explains the critical role of encryption within a defense-in-depth security strategy.\nKey topics include:\nHow encryption works and why strong key management is essential. AWS encryption services such as AWS KMS, AWS CloudHSM, and External Key Store (XKS). Encryption at rest, in transit, and in use (cryptographic computing). Post-quantum cryptography readiness and AWS support for ML-KEM, s2n-tls, and PQC-enabled services. Default S3 encryption, EBS envelope encryption, and always-on memory encryption on AWS Graviton chips. üëâ Blog 2 ‚Äì Proactive Strategies to Improve Network Resilience \u0026amp; Business Continuity on AWS This blog focuses on building cybersecurity resilience and business continuity for public sector organizations.\nHighlights include:\nApplying the NIST Cybersecurity Framework 2.0, AWS Well-Architected Framework, and AWS Security Reference Architecture (SRA). Why a multi-account strategy is essential, and how to centralize identity using AWS IAM Identity Center. Building infrastructure using Infrastructure as Code (IaC) with CloudFormation, CDK, SAM, or Terraform. Preparing recovery accounts, using consistent AZ IDs, and implementing multi-Region recovery. Creating backup strategies using AWS Backup, automated validation, immutable backups, and full workload recovery testing. üëâ Blog 3 ‚Äì Designing Serverless Integration Patterns for Large Language Models (LLMs) This blog explores best-practice patterns for integrating LLMs into serverless architectures.\nKey concepts include:\nCalling Amazon Bedrock directly from AWS Lambda, adjusting timeouts, and using streaming responses. Implementing prompt chaining with AWS Step Functions to avoid Lambda‚Äôs 15-minute timeout. Running multiple LLM tasks in parallel to reduce total workflow time by up to 37%. Implementing caching using DynamoDB or ElastiCache to reduce costs and improve performance. Choosing the right model and optimizing generative AI workflows (including RAG). "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"üéØ Week 3 Objectives Learn S3 storage fundamentals. Deploy static website on S3. Configure CloudFront CDN. Understand Edge Computing concepts. Explore Lightsail alternative hosting. üìÖ Tasks Completed Day Task Description Start End Reference Monday S3 bucket setup, policy \u0026amp; versioning 09/22 09/22 https://youtu.be/77_-eA9WjoI Tuesday Static website hosting with S3 09/23 09/23 https://youtu.be/8hK2yOub7Dk Wednesday CloudFront Distribution configuration 09/24 09/24 https://youtu.be/rZ2f9XQMtS4 Thursday Lambda@Edge basic use cases 09/25 09/25 https://aws.amazon.com/lambda/edge/ Friday Lightsail static hosting 09/26 09/26 https://aws.amazon.com/lightsail/ üß† Knowledge Gained S3 architecture \u0026amp; storage classes. Static hosting workflow. Global content delivery via CloudFront. Cache invalidation \u0026amp; distribution behavior. Edge Lambda usage for optimization. üõ† Practical Skills Deploy static website to S3. Configure CDN with HTTPS. Attach custom domain via CloudFront. Debug caching behaviour. ‚úîÔ∏è Week 3 Summary This week strengthened cloud storage and content delivery knowledge using S3 and CloudFront.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/3-blogstranslated/3.3-blog3/","title":"Blog 3 (English Version)","tags":[],"description":"","content":"Designing Serverless Integration Patterns for Large Language Models (LLMs) Author: Chris McPeek\nPublished: October 4, 2024\nCategories: Amazon Bedrock, AWS Lambda, AWS Step Functions, Serverless\nThis article includes contributions from Josh Hart (Principal Solutions Architect) and Thomas Moore (Senior Solutions Architect).\nThis article explores best-practice integration patterns for using large language models (LLMs) in serverless applications. These approaches help optimize performance, resource usage, and resiliency when integrating generative AI into your serverless architectures.\nOverview of Serverless, LLMs, and the Example Scenario Organizations of all sizes are now leveraging LLMs to build next-generation generative AI applications that unlock new customer experiences.\nServerless technologies such as:\nAWS Lambda AWS Step Functions Amazon API Gateway ‚Ä¶enable teams to go from idea to production quickly, without needing to manage servers. The pay-for-use billing model also supports flexible scaling with optimized cost.\nThe examples in this article use Amazon Bedrock, a fully managed service that provides access to foundation models (FMs) via API. Similar principles apply when using LLMs hosted with other services such as Amazon SageMaker.\nThe example scenario illustrates using an LLM to generate engaging marketing content for launching a new family SUV model. Images used in the marketing content were generated using Amazon Titan Image Generator, as shown below.\nDirect Invocation from AWS Lambda Calling Amazon Bedrock directly from Lambda The simplest serverless integration pattern is invoking Bedrock directly from a Lambda function using the AWS SDK.\nBelow is a Python example using boto3 to call the InvokeModel operation:\nimport json import boto3 brt = boto3.client(service_name=\u0026#39;bedrock-runtime\u0026#39;) def lambda_handler(event, context): body = json.dumps({ \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: 1000, \u0026#34;messages\u0026#34;: [{ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Create a 500 word car advert given these images and the following specification: \\n {}\u0026#34;.format(event[\u0026#39;spec\u0026#39;]) }, { \u0026#34;type\u0026#34;: \u0026#34;image\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;base64\u0026#34;, \u0026#34;media_type\u0026#34;: \u0026#34;image/jpeg\u0026#34;, \u0026#34;data\u0026#34;: event[\u0026#39;image\u0026#39;] } } ] }] }) modelId = \u0026#34;anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; response = brt.invoke_model( body=body, modelId=modelId, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34; ) response_body = json.loads(response.get(\u0026#34;body\u0026#34;).read()) return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: response_body[\u0026#34;content\u0026#34;][0][\u0026#34;text\u0026#34;] } "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"üéØ Week 4 Objectives Learn RDS relational database. Practice EC2 ‚Üî RDS connection. Understand DynamoDB NoSQL. Implement caching using Redis. Practice Networking workshops. üìÖ Tasks Completed Day Task Description Start End Reference Monday Create RDS MySQL instance 09/29 09/29 https://youtu.be/TKXGd5V5FqE Tuesday Connect EC2 ‚Üî RDS using VPC 09/30 09/30 https://youtu.be/jQnUFg4t0_o Wednesday DynamoDB table + PartiQL 10/01 10/01 https://youtu.be/y99YGaQjgxQ Thursday Redis ElastiCache setup 10/02 10/02 https://youtu.be/WLE8n0SdZ9o Friday Networking workshop 10/03 10/03 https://www.youtube.com/results?search_query=aws+network+workshop üß† Knowledge Gained RDS engine configuration and connection security. DynamoDB key structure \u0026amp; NoSQL principles. Redis caching strategies. VPC connectivity for databases. üõ† Practical Skills Create relational and NoSQL databases. Configure caching layer. Connect backend system with DB. ‚úîÔ∏è Week 4 Summary This week strengthened backend development with RDS, DynamoDB, and Redis on AWS.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"üéØ Week 5 Objectives Implement Auto Scaling. Configure Load Balancers. Monitor systems with CloudWatch. Use Cloud9 IDE. Deploy apps with Lightsail Containers. üìÖ Tasks Completed Day Task Description Reference Monday Create Auto Scaling Group + Launch Template https://youtu.be/DBouAHe6H7U Tuesday Application Load Balancer routing https://youtu.be/NfL-T0uTLlQ Wednesday CloudWatch metrics \u0026amp; dashboards https://youtu.be/I0lZt9CYyW8 Thursday AWS Lambda basics https://youtu.be/JIbIYZIeLqU Friday Lightsail Container deployment https://aws.amazon.com/lightsail/containers/ üß† Knowledge Gained Horizontal vs vertical scaling. Load balancing architectures. CloudWatch Logs, Metrics \u0026amp; Alarms. Serverless basics. üõ† Practical Skills Deploy scalable EC2 architecture. Configure ALB listener \u0026amp; target groups. Build Lambda function. Deploy containerized app on Lightsail. ‚úîÔ∏è Week 5 Summary Week 5 consolidated scaling and monitoring skills essential for modern cloud applications.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"üéØ Week 6 Objectives Understand High Availability architecture. Configure Route53 DNS. Learn AWS Managed Microsoft AD. Build final AWS project. Complete internship documentation. üìÖ Tasks Completed Day Task Description Reference Monday High Availability architecture (Multi-AZ) https://aws.amazon.com/architecture/ Tuesday Configure Route53 hosted zone \u0026amp; routing https://youtu.be/0tXUXcUqjKc Wednesday AWS Managed Microsoft AD integration https://aws.amazon.com/directoryservice/ Thursday Build final project (EC2 + RDS + S3 + CloudFront + ASG + Route53) ‚Äî Friday Write final report \u0026amp; presentation slides ‚Äî üß† Knowledge Gained HA vs Fault Tolerance. Multi-AZ design pattern. Enterprise identity integration (Microsoft AD). DNS routing strategies. üõ† Practical Skills Build full cloud architecture: EC2 RDS S3 CloudFront Auto Scaling Route53 Prepare technical documentation \u0026amp; presentation. ‚úîÔ∏è Week 6 Summary This final week combined all AWS skills learned into a complete, production-ready cloud architecture.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"üéØ Week 7 Objectives Research application requirements for Aurora Calendar. Study Amplify Hosting and authentication flows. Learn Cognito User Pools \u0026amp; JWT tokens. Learn API Gateway HTTP APIs for serverless backend. Start drafting the full system architecture. üìÖ Tasks Completed Day Task Description Reference Monday Define Aurora project scope, user stories, and core features ‚Äî Tuesday Study AWS Amplify Hosting, frontend deployment workflow https://docs.amplify.aws Wednesday Learn Cognito (signup, signin, refresh token, secure API) https://youtu.be/LG2-lUyJp4M Thursday Learn API Gateway HTTP APIs + authorizer https://youtu.be/8rj8k0qQCo0 Friday Create initial system architecture diagram (v1) ‚Äî üß† Knowledge Gained Authentication vs Authorization. Cognito JWT-based authentication flow. IAM-based API authorization. Amplify build pipeline. High-level serverless architecture design. üõ† Practical Skills Deploy frontend with Amplify Console. Use Cognito Hosted UI. Create User Pools. Connect Amplify ‚Üí API Gateway. ‚úîÔ∏è Week 7 Summary A strong foundation for Aurora Calendar\u0026rsquo;s authentication and frontend hosting was established this week.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"üéØ Week 8 Objectives Learn DynamoDB design (PK/SK model). Implement CRUD Lambda functions for events. Build API Gateway ‚Üí Lambda ‚Üí DynamoDB pipeline. Learn EventBridge scheduling for calendar events. üìÖ Tasks Completed Day Task Description Reference Monday DynamoDB basics \u0026amp; table design decisions https://youtu.be/y99YGaQjgxQ Tuesday Create DynamoDB table for events (PK: userId, SK: eventId) ‚Äî Wednesday Write Lambda functions for Create/Read/Update/Delete events https://youtu.be/JIbIYZIeLqU Thursday EventBridge scheduler research (Cron, rules, targets) https://youtu.be/eI3W6CXYhYw Friday Implement scheduled triggers for reminders ‚Äî üß† Knowledge Gained NoSQL modeling vs SQL. Designing scalable partition keys. Lambda execution model \u0026amp; cold starts. EventBridge scheduled events. üõ† Practical Skills Define DynamoDB table schema. Build serverless CRUD API. Build scheduled reminder triggers. Log events with CloudWatch Logs. ‚úîÔ∏è Week 8 Summary Aurora Calendar backend was implemented: events API, scheduling logic, and DynamoDB integration.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"üéØ Week 9 Objectives Implement application logging. Learn CloudWatch Logs \u0026amp; Metrics. Trace Lambda performance \u0026amp; throttling. Learn serverless debugging workflow. üìÖ Tasks Completed Day Task Description Reference Monday Study CloudWatch logging structure https://youtu.be/I0lZt9CYyW8 Tuesday Add structured JSON logs in Lambda ‚Äî Wednesday Setup CloudWatch Metric Filters \u0026amp; Alarms https://youtu.be/vn2-HTfyhV8 Thursday Debug API errors using CloudWatch Insights ‚Äî Friday Optimize Lambda performance using logs ‚Äî üß† Knowledge Gained Logging best practices. Metric filters \u0026amp; monitoring alarms. Performance tuning for Lambda. üõ† Practical Skills Write structured logs. Monitor errors \u0026amp; throttles. Improve cold start performance. ‚úîÔ∏è Week 9 Summary This week improved Aurora Calendar reliability through monitoring and performance logging.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"üéØ Week 10 Objectives Learn AWS CI/CD tools. Build automated deployment pipeline. Deploy backend infrastructure using CloudFormation. Automate build \u0026amp; test steps. üìÖ Tasks Completed Day Task Description Reference Monday Learn CodePipeline concepts https://youtu.be/M8FJ5D7C5DI Tuesday Setup GitHub ‚Üí CodePipeline integration ‚Äî Wednesday Build backend package with CodeBuild https://youtu.be/L4SV-UjaFso Thursday Generate CloudFormation template for resources https://aws.amazon.com/cloudformation/ Friday Full CI/CD deployment test ‚Äî üß† Knowledge Gained Continuous Delivery workflow. How CodePipeline stages work. How CodeBuild builds Lambda artifacts. Infrastructure as Code (IaC) with CloudFormation. üõ† Practical Skills Push code ‚Üí auto deploy Lambda/API changes. Write buildspec.yml for Node.js Lambda build. Deploy stacks using CloudFormation. ‚úîÔ∏è Week 10 Summary Aurora Calendar now has a fully automated CI/CD pipeline for backend deployment.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"üéØ Week 11 Objectives Integrate Resend Email API. Send email reminders via EventBridge ‚Üí Lambda ‚Üí Resend. Improve UI/UX for Aurora Calendar. Add missing backend logic. üìÖ Tasks Completed Day Task Description Reference Monday Study Resend API \u0026amp; authentication https://resend.com Tuesday Build Lambda email sender ‚Äî Wednesday Connect EventBridge reminders ‚Üí email workflow ‚Äî Thursday Add UI components for events dashboard ‚Äî Friday Finalize all event-related backend endpoints ‚Äî üß† Knowledge Gained Email delivery workflow. API key security practices. Event-driven architecture patterns. üõ† Practical Skills Integrate 3rd-party email system. Schedule and send automated reminders. Implement React UI with Amplify hosting. ‚úîÔ∏è Week 11 Summary Aurora Calendar gained a complete reminder workflow, including automated emails.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"üéØ Week 12 Objectives Prepare final report \u0026amp; proposal. Test full system flow. Optimize performance \u0026amp; cost. Write final architecture documentation. Prepare presentation slides. üìÖ Tasks Completed | Day | Task Description | Reference | |\u0026mdash;\u0026ndash;|\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;|\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;|5e | Monday | Full application testing (Frontend + API + Email) | ‚Äî | | Tuesday | Performance checks \u0026amp; CloudWatch tracing | ‚Äî | | Wednesday | Write Project Proposal | ‚Äî | | Thursday | Write Architecture Documentation | ‚Äî | | Friday | Create presentation for final submission | ‚Äî |\nüß† Knowledge Gained System testing strategy. Architecture optimization. Documentation structure for AWS projects. üõ† Practical Skills Final debugging and polishing. Technical writing for cloud applications. Presentation design for AWS architecture. ‚úîÔ∏è Week 12 Summary The Aurora Calendar System is completed with full documentation, architecture diagrams, CI/CD pipeline, and a fully functional product.\n"},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://tranhoangtrunghieu.github.io/fcj-workshop-template-main/tags/","title":"Tags","tags":[],"description":"","content":""}]